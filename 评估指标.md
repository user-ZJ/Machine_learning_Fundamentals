# 机器学习中评估指标

## 回归问题评估指标
1. 平均绝对值误差MAE（Mean Absolute Error）  
> 平均绝对误差MAE（Mean Absolute Error）又被称为 L1范数损失。  
> ![](https://i.imgur.com/STI1GGN.png)  
>   > MAE虽能较好衡量回归模型的好坏，但是绝对值的存在导致函数不光滑，在某些点上不能求导，可以考虑将绝对值改为残差的平方，这就是均方误差。  

2. 均方误差MSE  
> 均方误差MSE（Mean Squared Error）又被称为 L2范数损失 。  
> ![](https://i.imgur.com/hBfFaCf.png)  
>   > 由于MSE与我们的目标变量的量纲不一致，为了保证量纲一致性，我们需要对MSE进行开方  

3. 均方根误差RMSE  
> 均方根误差(Root Mean Square Error)  
> ![](https://i.imgur.com/0BeP9yT.png)  

4. 平均绝对百分误差MAPE/MAPD  
> MAPE (Mean Absolute Percentage Error, 也叫mean absolute percentage deviation (MAPD)不仅仅考虑预测值与真实值的误差，还考虑了误差与真实值之间的比例，在某些场景下，比如房价从0.5w到5w之间，0.5预测成1.0与5.0预测成4.5的差距是非常大的，在一些竞赛当中，MAPE也是常用的目标函数之一。  
> ![](https://i.imgur.com/UnkckGP.png)  

5. 决定系数R^2
> 决定系数就是一个无量纲化的指标,决定系数又称为R^2 score，反映因变量的全部变异(变化)能通过回归关系被自变量解释的比例。  
> ![](https://i.imgur.com/Xu1ElXw.png)  
> 化简上面的公式 ,分子就变成了我们的均方误差MSE，下面分母就变成了方差:  
> ![](https://i.imgur.com/AMVc6uv.png)  
> R^2在0-1之间，R^2越大，表示模型拟合效果越好  

>以上的评估指标（除MAPE外）是基于误差的均值对进行评估的，均值对异常点（outliers）较敏感，如果样本中有一些异常值出现，会对以上指标的值有较大影响。  
>  > 通常用一下两种方法解决评估指标的鲁棒性问题:  
>  > * 剔除异常值  
>  > * 设定一个相对误差 ，当该值超过一定的阈值时，则认为其是一个异常点，剔除这个异常点，将异常点剔除之后。再计算平均误差来对模型进行评价  
>  > * 使用误差的分位数来代替,如利用中位数来代替平均数。例如 MAPE  

## 分类问题评估指标
1. 精度 Accuracy  
> 预测正确的样本的占总样本的比例，取值范围为[0,1]，取值越大，模型预测能力越好。  
> ![](https://i.imgur.com/fuqIhKB.png)  
> 精度评价指标平等对待每个类别，即每一个样本判对 (1) 和判错 (0) 的代价都是一样的。  
>   > * 对于有倾向性的问题，往往不能用精度指标来衡量。比如，判断空中的飞行物是导弹还是其他飞行物，很显然为了减少损失，我们更倾向于相信是导弹而采用相应的防护措施。此时判断为导弹实际上是其他飞行物与判断为其他飞行物实际上是导弹这两种情况的重要性是不一样的；  
>   > * 对于样本类别数量严重不均衡的情况，也不能用精度指标来衡量。比如银行客户样本中好客户990个，坏客户10个。如果一个模型直接把所有客户都判断为好客户，得到精度为99%，但这显然是没有意义的。  

2. 混淆矩阵 Confusion Matrix  
> ![](https://i.imgur.com/u7eVf5D.png)  
> * **准确率（查准率） Precision**  
> Precision 是分类器预测的正样本中预测正确的比例，取值范围为[0,1]，取值越大，模型预测能力越好。  
> ![](https://i.imgur.com/ogdfGni.png)  
> * **召回率（查全率）Recall**  
> Recall 是分类器所预测正确的正样本占所有正样本的比例，取值范围为[0,1]，取值越大，模型预测能力越好。  
> ![](https://i.imgur.com/IWXOv4z.png)  
> > * Precision高的模型为“宁放过一万，不错拿一个”，“疑罪从无”，分类阈值较高
> > * Recall高的模型为“宁错拿一万，不放过一个”，分类阈值较低
> 
> 随着阈值的变化Recall和Precision往往会向着反方向变化，这种规律很难满足我们的期望，即Recall和Precision同时增大。  

3. Fβ Score  
> Precision和Recall 是互相影响的，理想情况下肯定是做到两者都高，但是一般情况下Precision高、Recall 就低， Recall 高、Precision就低。为了均衡两个指标，我们可以采用Precision和Recall的加权调和平均（weighted harmonic mean）来衡量，即Fβ Score，公式如下：  
> ![](https://i.imgur.com/izgJ6wk.png)  
> β表示权重,β>0度量了查全率对查准率的相对重要性，β=1时退化为标准的F1，β>1时查全率影响更大，β<1时查准率影响更大。  

4. ROC和AUC  
> ROC曲线为 FPR 与 TPR 之间的关系曲线，这个组合以 FPR 对 TPR，即是以代价 (costs) 对收益 (benefits)，显然收益越高，代价越低，模型的性能就越好。  
> x 轴为假阳性率（FPR）：在所有的负样本中，分类器预测错误的比例  
> ![](https://i.imgur.com/FK0h8C4.png)  
> y 轴为真阳性率（TPR）：在所有的正样本中，分类器预测正确的比例（等于Recall）  
> ![](https://i.imgur.com/2DvS36C.png)  
>
> AUC 值为 ROC 曲线所覆盖的区域面积，显然，AUC越大，分类器分类效果越好。AUC = 1，是完美分类器。
0.5 < AUC < 1，优于随机猜测。有预测价值。AUC = 0.5，跟随机猜测一样（例：丢铜板），没有预测价值。AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。  

5. KS Kolmogorov-Smirnov
KS值是在模型中用于区分预测正负样本分隔程度的评价指标，一般应用于金融风控领域。  
在KS曲线中，则是以阈值作为横坐标，以FPR和TPR作为纵坐标，ks曲线则为TPR-FPR，ks曲线的最大值通常为ks值。  

## 聚类问题评估指标
1. 兰德指数  
2. 互信息  
3. 轮廓系数  

## 深度学习评估指标
一般深度学习以准确率来评估模型好坏，如分类问题，会看top1、top3、top5分类准确率。

reference：
https://zhuanlan.zhihu.com/p/36305931









  
