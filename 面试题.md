1. 深度学习中超参数有哪些？  
优化器超参数：learning_rate、minibatch_size、epochs  
模型超参数：层数量（number of layer）、隐藏单元数量（number of hidden units）、RNN超参数  
学习率过低会导致模型收敛太慢，学习率过高会导致模型来回震荡；minibatch size太小会使训练速度变慢，minibatch size太大会使计算成本过高并降低准确度，所以一般minibatch size选择32至256之间（Systematic evaluation of CNN advances on the ImageNet ）；epochs一般使用早停方法来设置模型训练次数，防止模型过拟合；模型隐藏单元越多，层数越多学习能力越强，不过也容易导致过拟合，对于卷积层，层数越多效果越好，不过对于全链接层一般3 层神经网络的性能通常优于 2 层神经网络，但是更深帮助不大；RNN超参数有cell类型、模型深度、单词嵌入大小

2. 在训练模型过程中遇到哪些问题，怎么解决的？  
	* 训练集上loss/准确率来回震荡而不下降；学习率设置过高导致的问题，降低学习率进行训练。  
	* 训练集 和验证集loss 都比较大；一般是欠拟合，增加网络层数、增加训练epochs    
	* 使用logloss作为损失函数，容易产生拟合，cross_entropy则不会  

3. 导致过拟合的原因有哪些？有哪些解决方案？  
原因：  
	（1）. 训练集的数量级和模型的复杂度不匹配，训练集的数量级要小于模型的复杂度   
	（2）. 训练集和测试集特征分布不一致  
	（3）. 样本里的噪音数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系；  
	（4）. 权值学习迭代次数足够多(Overtraining)，拟合了训练数据中的噪声和训练样例中没有代表性的特征  
解决方案：  
	(1). 降低模型复杂度（缩小宽度和减小深度），使其适合自己训练集的数量级  
	(2). 增加数据规模（采集数据、对数据进行增强，如对图像旋转，缩放，剪切，添加噪声等）  
	(3). 对数据做归一化，防止某些特征主导参数优化过程  
	(4). 预先对数据进行清洗，剔除异常值，处理缺失值    
	(5). 添加正则化项  
	L0范数是指向量中非0的元素的个数，L1范数是指向量中各个元素绝对值之和，L2范数是指向量各元素的平方和然后求平方根。   
	(6). 添加dropout、dropconnect和dropblock层  
	(7). 使用早停法，避免模型训练迭代次数过多   
	(8). 使用集成方法，融合多个模型  
	(9). weight decay（权值衰减）  
	(10). 交叉验证  

4. 导致欠拟合的原因有哪些？有哪些解决方案？  
原因：  
	(1). 模型复杂度过低  
	(2). 特征量过少  
解决方案：  
	(1). 增加网络复杂度  
	(2). 增加训练次数  
	(3). 增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间；  
	(4). 添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强；  
	(5). 减少正则化参数，正则化的目的是用来防止过拟合的，但是模型出现了欠拟合，则需要减少正则化参数；  
	(6). 使用非线性模型，比如核SVM 、决策树、深度学习等模型；  
	(7). 调整模型的容量(capacity)，通俗地，模型的容量是指其拟合各种函数的能力。  
	(8). 容量低的模型可能很难拟合训练集；使用集成学习方法，如Bagging ,将多个弱学习器Bagging。  

5. 图片数据预处理的方式有哪些？
图片预处理一般是对像素进行归一化；不同模型的归一化方式不同，VGG和ResNet是减去训练集均值，Xception和InceptionV3是线性缩放到[-1,1]  

6. dropout和batch normalization区别  
dropout:  
	dropout主要是一种正规化技术。 它将噪声引入神经网络，迫使神经网络学会很好地推广以处理噪声。 （这是一个很大的过度简化，dropout实际上不仅仅是对噪音的鲁棒性）  
	Dropout以概率p关闭神经元,关闭的神经元在训练阶段的前向传播和后向传播阶段都不起作用：因为这个原因，每当一个单一的神经元被丢弃时，训练阶段就好像是在一个新的神经网络上完成。所欲Dropout可以被认为是集成非常多的大神经网络的实用Bagging方法,提供了一种廉价的Bagging集成近似,能够训练和评估指数级的神经网络。  
batch normalization:  
	批量标准化主要是用于改进参数优化的技术，用于加速训练过程，但噪音具有正则化的效果,有时使Dropout变得没有必要。   

7. 请解释偏差和方差  
偏差：预测值偏离标签程度  
方差：数据波动对误差的影响  

8. 什么是维度灾难？如何优化？  
维度灾难：  
	(1). 维度增加，样本变得稀疏，容易找到超平面对样本进行划分，但映射回原空间容易发生过拟合   
	(2). 高维空间中距离逐渐失效（欧式距离最大值和最小值趋近相同）  
优化方法：  
	(1). 从N个特征中选取M个特征用来分类，用启发方法（贪心、穷举）确定最佳特征组合  
	(2). PCA降维  
	(3). 增加训练数据  

9. 降维的好处  
(1). 减少存储空间  
(2). 几块计算速度  
(3). 去除冗余特征  
(4). 可视化  

10. 卷积层对全连接层优势  
(1). 保留空间特性
(2). 平移不变性  
(3). 权重共享  
(4). 局部连接  

11. 不平衡数据集处理  
(1). 给数据不足的类别在损失函数中赋予更高的权重    
(2). 过采样，对数据不足的类进行重复采样（可用数据较少时）  
(3). 欠采样，简单跳过数据较多的类的实例（可用数据较多时）  
(4). 增加数据（数据增强、采集新数据）  

12. 小卷积相对大卷积核的优势  
(1). 减少计算量  
(2). 特征提取更细  


