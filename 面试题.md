1. 深度学习中超参数有哪些？  
优化器超参数：learning_rate、minibatch_size、epochs  
模型超参数：层数量（number of layer）、隐藏单元数量（number of hidden units）、RNN超参数  
学习率过低会导致模型收敛太慢，学习率过高会导致模型来回震荡；minibatch size太小会使训练速度变慢，minibatch size太大会使计算成本过高并降低准确度，所以一般minibatch size选择32至256之间（Systematic evaluation of CNN advances on the ImageNet ）；epochs一般使用早停方法来设置模型训练次数，防止模型过拟合；模型隐藏单元越多，层数越多学习能力越强，不过也容易导致过拟合，对于卷积层，层数越多效果越好，不过对于全链接层一般3 层神经网络的性能通常优于 2 层神经网络，但是更深帮助不大；RNN超参数有cell类型、模型深度、单词嵌入大小

2. 在训练模型过程中遇到哪些问题，怎么解决的？  
	* 训练集上loss/准确率来回震荡而不下降；学习率设置过高导致的问题，降低学习率进行训练。  
	* 训练集 和验证集loss 都比较大；一般是欠拟合，增加网络层数、增加训练epochs    
	* 使用logloss作为损失函数，容易产生拟合，cross_entropy则不会  

3. 导致过拟合的原因有哪些？有哪些解决方案？  
原因：  
	（1）. 训练集的数量级和模型的复杂度不匹配，训练集的数量级要小于模型的复杂度   
	（2）. 训练集和测试集特征分布不一致  
	（3）. 样本里的噪音数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系；  
	（4）. 权值学习迭代次数足够多(Overtraining)，拟合了训练数据中的噪声和训练样例中没有代表性的特征  
解决方案：  
	(1). 降低模型复杂度（缩小宽度和减小深度），使其适合自己训练集的数量级  
	(2). 增加数据规模（采集数据、对数据进行增强，如对图像旋转，缩放，剪切，添加噪声等）  
	(3). 对数据做归一化，防止某些特征主导参数优化过程  
	(4). 预先对数据进行清洗，剔除异常值，处理缺失值    
	(5). 添加正则化项  
	L0范数是指向量中非0的元素的个数，L1范数是指向量中各个元素绝对值之和，L2范数是指向量各元素的平方和然后求平方根。   
	(6). 添加dropout、dropconnect和dropblock层  
	(7). 使用早停法，避免模型训练迭代次数过多   
	(8). 使用集成方法，融合多个模型  
	(9). weight decay（权值衰减）  
	(10). 交叉验证  

4. 导致欠拟合的原因有哪些？有哪些解决方案？  
原因：  
	(1). 模型复杂度过低  
	(2). 特征量过少  
解决方案：  
	(1). 增加网络复杂度  
	(2). 增加训练次数  
	(3). 增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间；  
	(4). 添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强；  
	(5). 减少正则化参数，正则化的目的是用来防止过拟合的，但是模型出现了欠拟合，则需要减少正则化参数；  
	(6). 使用非线性模型，比如核SVM 、决策树、深度学习等模型；  
	(7). 调整模型的容量(capacity)，通俗地，模型的容量是指其拟合各种函数的能力。  
	(8). 容量低的模型可能很难拟合训练集；使用集成学习方法，如Bagging ,将多个弱学习器Bagging。  

5. 图片数据预处理的方式有哪些？
图片预处理一般是对像素进行归一化；不同模型的归一化方式不同，VGG和ResNet是减去训练集均值，Xception和InceptionV3是线性缩放到[-1,1]  

6. dropout和batch normalization区别  
dropout:  
	dropout主要是一种正规化技术。 它将噪声引入神经网络，迫使神经网络学会很好地推广以处理噪声。 （这是一个很大的过度简化，dropout实际上不仅仅是对噪音的鲁棒性）  
	Dropout以概率p关闭神经元,关闭的神经元在训练阶段的前向传播和后向传播阶段都不起作用：因为这个原因，每当一个单一的神经元被丢弃时，训练阶段就好像是在一个新的神经网络上完成。所欲Dropout可以被认为是集成非常多的大神经网络的实用Bagging方法,提供了一种廉价的Bagging集成近似,能够训练和评估指数级的神经网络。  
batch normalization:  
	批量标准化主要是用于改进参数优化的技术，用于加速训练过程，但噪音具有正则化的效果,有时使Dropout变得没有必要。   

7. 请解释偏差和方差  
偏差：预测值偏离标签程度  
方差：数据波动对误差的影响  

8. 什么是维度灾难？如何优化？  
维度灾难：  
	(1). 维度增加，样本变得稀疏，容易找到超平面对样本进行划分，但映射回原空间容易发生过拟合   
	(2). 高维空间中距离逐渐失效（欧式距离最大值和最小值趋近相同）  
优化方法：  
	(1). 从N个特征中选取M个特征用来分类，用启发方法（贪心、穷举）确定最佳特征组合  
	(2). PCA降维  
	(3). 增加训练数据  

9. 降维的好处  
(1). 减少存储空间  
(2). 几块计算速度  
(3). 去除冗余特征  
(4). 可视化  

10. 卷积层对全连接层优势  
(1). 保留空间特性
(2). 平移不变性  
(3). 权重共享  
(4). 局部连接  

11. 不平衡数据集处理  
(1). 给数据不足的类别在损失函数中赋予更高的权重    
(2). 过采样，对数据不足的类进行重复采样（可用数据较少时）  
(3). 欠采样，简单跳过数据较多的类的实例（可用数据较多时）  
(4). 增加数据（数据增强、采集新数据）  

12. 小卷积相对大卷积核的优势  
(1). 减少计算量  
(2). 特征提取更细  

13. 什么是伪标签技术  
* 伪标签技术是将test数据集中的数据加入到train数据集中，其对应的标签为基于原有数据集训练好的模型预测得到的  
* 伪标签技术在一定程度上起到一种正则化的作用。如果训练开始就直接使用该技术，则网络可能会有过拟合风险，但是如果经过几轮训练迭代后（只是用原有训练集数据）将训练集和未打标签的数据一起进行训练，则会提升网络的泛化能力  
* 操作过程中一般每个batch中的1/4到1/3的数据为伪标签数据  

14. group convolution  
group convolution是对卷积进行分组的技术，可以有效减少计算量  
group convolution认为卷积操作把所有的通道信息考虑在内，可能是一种信息浪费，卷积参数有限，产生的结构难免会有一定相关性，有可能造成过拟合（类似于全连接层）  

15. 如何理解核函数  
* 核函数并不是一种映射，它只是用来计算映射到高维空间后数据内积的一种方法  
* 在低维空间中线性不可分的数据集在高维空间中存在线性可分的超平面（这点可以和维数灾难中高维空间形成的分类器相当于低维空间中一个非常复杂的非线性分类器产生过拟合联系起来），但在特征空间中仍要计算内积，因此在原计算式子中代入的不再是x而是φ(x)。计算核函数K时不需要知道具体φ(x)的定义和计算过程。核函数定义为（满足一定条件）  
K(x1, x2) = φ(x1)'φ(x2)  

16. 超参数搜索方法  
* 网格搜索：在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果  
* 贝叶斯优化：贝叶斯优化其实就是在函数方程不知的情况下根据已有的采样点预估函数最大值的一个算法。该算法假设函数符合高斯过程(GP)  
* 随机搜索：已经发现，简单地对参数设置进行固定次数的随机搜索，比在穷举搜索中的高维空间更有效。 这是因为事实证明，一些超参数不通过特征变换的方式把低维空间转换到高维空间，而在低维空间不可分的数据，到高维空间中线性可分的几率会高一些。具体方法：核函数，如：高斯核，多项式核等等。  
* 基于梯度：计算相对于超参数的梯度，然后使用梯度下降优化超参数。  

17. Boosting和Bagging  
* Boosting主要思想是将一族弱学习器提升为强学习器的方法，具体原理为：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，来训练下一个基学习器，如此重复进行直至学习器数目达到了事先要求，最终进行学习器的加权结合  
* Bagging是并行式集成学习方法最著名的代表，具体做法是对样本进行有放回的采样，然后基于每个采样训练集得到一个基学习器，再将它们进行组合。在预测时，对于分类任务通常使用简单投票法，对回归任务使用简单平均法  

18. mean pooling和max pooling的反向传播   
主要原则是需要保证传递的loss（或者梯度）总和不变。  
* mean pooling的前向传播就是把一个patch中的值求取平均来做pooling，那么反向传播的过程也就是把某个元素的梯度等分为n份分配给前一层，这样就保证池化前后的梯度（残差）之和保持不变  
* max pooling的前向传播是把patch中最大的值传递给后一层，而其他像素的值直接被舍弃掉。那么反向传播也就是把梯度直接传给前一层某一个像素，而其他像素不接受梯度，也就是为0   
max pooling操作和mean pooling操作不同点在于需要记录下池化操作时到底哪个像素的值是最大，也就是max id，这个变量就是记录最大值所在位置的，因为在反向传播中要用到  

19. 激活函数作用  
https://github.com/user-ZJ/deep-learning/blob/master/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.md  

20. 池化的作用  
减少特征维度大小，使特征更加可控；  
减少参数个数，从而控制过拟合程度；  
增加网络对略微变换后的图像的鲁棒性；  
达到一种尺度不变性，即无论物体在图像中哪个方位均可以被检测到  

21. 1x1卷积核的作用  
* 1x1卷积来做channel reduction，通过减少通道数，减少计算量。如input是24x24x3,使用两个3x3x128的卷积，计算量为24x24x3x3x3x128+24x24x128x3x3x128;使用3x3x128,1x1X64,3X3X128的卷积，计算量为24x24x3x3x3x128+24x24x128x1x1x64+24x24x64x3x3x128.  
* 1x1卷积可以替代全连接层，且不会限制网络输入的大小，全链接层的input大小为固定的，所以要求整个网络输入时固定大小    
* 对不同特征进行归一化操作  
* 用于不同channel上特征的融合  



