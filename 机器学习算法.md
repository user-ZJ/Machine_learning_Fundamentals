线性回归:使用直线或超平面拟合数据，预测位置数据的值

逻辑回归:将线性回归结果映射到0-1之间

正则化：是其他算法（通常是回归算法）的延伸，根据算法的复杂度对算法进行调整，对复杂模型进行惩罚  
L1正则化 L2正则化  

最小二乘法：令算式函数对每个参数偏导数为0，直接求出各个参数。

梯度下降法：设置一个步长，更新参数为（原始值-（步长*偏导数）），反复迭代。


分类及回归树（CART）：使用**基尼指数**来选择属性划分，选择基尼指数最小的属性作为最优划分属性。基尼指数反映从数据集中随机抽取两个样本，其类别标记不一致的概率。  
ID3 (Iterative Dichotomiser 3)决策树：以**信息熵的下降速度**为选取测试属性的标准，即在每个节点选取还尚未被用来划分的具有**最高信息增益**的属性作为划分标准，然后继续这个过程，直到生成的决策树能完美分类训练样例  
C4.5决策树：通过信息增益率选择分裂属性；先从候选划分属性中招数信息增益高于平均水平的属性，再从中选择增益率最高的。  
随机森林：随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树，每棵决策树都是一个分类器（假设现在针对的是分类问题），那么对于一个输入样本，N棵树会有N个分类结果。而随机森林集成了所有的分类投票结果，将投票次数最多的类别指定为最终的输出

朴素贝叶斯算法：满足“属性条件独立假设”的贝叶斯分类器,贝叶斯定理根据某些其他事件（在此例中是信息被分类为垃圾信息）的联合概率分布计算某个事件（在此例中是信息为垃圾信息）的发生概率。  

支持向量机：是一种十分常见的分类器(二类分类)，通过构造分割面将数据进行分离，基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解  
SMO(Sequential Minimal Optimization)顺序最小优化。  


k-Nearest Neighbor(KNN)：监督学习中的分类方法，对于一个样本x，要给它分类，即求出它的y，就从数据集中，在x附近找离它最近的K个数据点，这K个数据点，类别c占的个数最多，就把x的label设为c。  


k-Means算法:非监督学习中聚类算法，K是人工固定好的数字，假设数据集合可以分为K个簇，由于是依靠人工定好，需要一点先验知识

感知器神经网络：是一种前馈人工神经网络模型，其将输入的多个数据集映射到单一的输出的数据集上，可以解决任何线性可分问题

BP（Back Propagation）算法：BP算法全称叫作误差反向传播(error Back Propagation，或者也叫作误差逆传播)算法。其算法基本思想为：在2.1所述的前馈网络中，输入信号经输入层输入，通过隐层计算由输出层输出，输出值与标记值比较，若有误差，将误差反向由输出层向输入层传播，在这个过程中，利用梯度下降算法对神经元权值进行调整。更新参数：w=w-学习率*总误差对w的偏导数。  
BP算法中核心的数学工具就是微积分的链式求导法则。  


Boosting:Boosting方法是一种用来提高弱分类算法准确度的方法,这种方法通过构造一个预测函数系列,然后以一定的方式将他们组合成一个预测函数。  Boosting是一种框架算法，拥有系列算法，如AdaBoost，GradientBoosting，LogitBoost等算法。  Boosting中所有的弱分类器可以是不同类的分类器。  
AdaBoost：若为Adaboost分类，函数模型使用CART分类树；若为Adaboost回归，函数模型使用CART回归树。损失函数为“指数损失函数”。

Bootstrapped Aggregation（Bagging）：弱学习器通过投票的方式，决定样本分类结果    
 



标准化：





CTC

支持向量机和逻辑回归区别：



拉格朗日乘子法：其主要思想是将约束条件函数与原函数联立，从而求出使原函数取得极值的各个变量的解。

